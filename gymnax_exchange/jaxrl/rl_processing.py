import jax
import jax.numpy as jnp

#from utils.jstring import JString

import distrax
import optax

PAD_FLAG = 0
OBS_FLAG = 1
ACT_FLAG = 2

def layer_norm(x, w, eps=1e-5):
    mean = jnp.mean(x, axis=-1, keepdims=True)
    var = jnp.var(x, axis=-1, keepdims=True)
    std = jnp.sqrt(var + eps)
    return (x - mean) / std * w['weight'] + w['bias']

def get_ppo_agent(RWKV, params, seed=0):
    key = jax.random.key(seed)

    fan_out = params['head']['weight'].shape[-1]
    fan_in = 1
    a = jnp.sqrt(6/(fan_in + fan_out))
    params['value_head'] = {'weight': jax.random.uniform(key, shape=(fan_out,), dtype=params['head']['weight'].dtype, minval=-a, maxval=a)}
    
    def forward(tokens, state, params, length=None):
        tokens = jnp.array(tokens).reshape((-1,))
        if length is None:
            length = jnp.size(tokens)
        x = params['emb']['weight'][tokens]
        x, state = RWKV.forward_seq(x, state, params, length)

        x_ln = layer_norm(x, params['ln_out'])
        
        x = x_ln @ params['head']['weight'].T
        v = x_ln @ params['value_head']['weight']
        return x, v, state

    return forward, params

"""
buf -> contains all tokens
flags -> 0 for pad, 1 for observations, 2 for actions (generated by LLM)
rewards -> per-token rewards
values -> per-token values

flags = [1 1 1 1 1 2 2 0]
values -> direct outputs of LLM
actions -> predicting *next* token
reward is aligned to last action
"""

def calculate_gae(flags, dones, values, rewards, last_val, gamma, gae_lambda):
    def _get_advantages(gae_and_next_value, transition):
        # TODO: prevent gamma decay on "observations"
        gae, next_value = gae_and_next_value
        flag, done, value, reward = transition
        done = jnp.where(jnp.logical_or(done, flag == PAD_FLAG), 1.0, 0.0)
        delta = reward + gamma * next_value * (1 - done) - value
        gae = delta + gamma * gae_lambda * (1 - done) * gae
        return (gae, value), gae

    _, advantages = jax.lax.scan(
        _get_advantages,
        (jnp.zeros_like(last_val), last_val),
        (flags, dones, values, rewards),
        reverse=True
    )
    return advantages, advantages + values

"""
buf -> contains all tokens
flags -> 0 for pad, 1 for observations, 2 for actions (generated by LLM)
o_values -> per-token rewards
o_log_probs -> per-token log probs (shifted by 1, so [1, 2, 3, 4, ...])
gae -> per-token advantages
targets -> per-token targets
state -> inital state
"""

def ppo_loss(params, forward_fn, config, buf, flags, o_values, o_log_prob, gae, targets, state):
    gae = gae[..., 1:]
    pre_action_flags = (flags[..., 1:] == ACT_FLAG)
    total_actions = jnp.sum(jnp.astype(pre_action_flags, jnp.float32))
    
    pi, value, state = forward_fn(buf.tokens, state, params, buf.length)
    pi = distrax.Categorical(logits=pi[..., :-1, config["MIN_ACTION_TOK"]:config["MAX_ACTION_TOK"] + 1])  # [1, 2, 3, 4, ...]
    log_prob = pi.log_prob(buf.tokens[..., 1:] - config["MIN_ACTION_TOK"])  # [1, 2, 3, 4, ...]

    
    value_pred_clipped = o_values + (value - o_values).clip(-config["CLIP_EPS"], config["CLIP_EPS"])
    value_losses = jnp.square(value - targets)
    value_losses_clipped = jnp.square(value_pred_clipped - targets)
    # value_loss = jnp.sum(jnp.where(pre_action_flags, 0.5 * jnp.maximum(value_losses, value_losses_clipped)[..., :-1], 0.0)) / total_actions
    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()

    ratio = jnp.exp(log_prob - o_log_prob)  # [1, 2, 3, 4, ...]
    gae_nan = jnp.where(pre_action_flags, gae, jnp.nan)
    gae = (gae - jnp.nanmean(gae_nan)) / (jnp.nanstd(gae_nan) + 1e-8)
    loss_actor1 = ratio * gae
    loss_actor2 = jnp.clip(ratio, 1.0 - config["CLIP_EPS"], 1.0 + config["CLIP_EPS"]) * gae
    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)
    # loss_actor = loss_actor.mean() # change
    loss_actor = jnp.sum(jnp.where(pre_action_flags, loss_actor, 0.0)) / total_actions
    
    # entropy = pi.entropy().mean() # change
    entropy = jnp.sum(jnp.where(pre_action_flags, pi.entropy(), 0.0)) / total_actions
    total_loss = (loss_actor + config["VF_COEF"] * value_loss - config["ENT_COEF"] * entropy)
    return total_loss, (value_loss, loss_actor, entropy, state)


ppo_grad = jax.value_and_grad(ppo_loss, has_aux=True)

def ppo_update(solver, forward_fn, params, optimizer, buf, flags, o_values, o_log_prob, gae, targets, state, config):
    (loss, (value_loss, loss_actor, entropy, state)), grads = ppo_grad(params, forward_fn, config, buf, flags, o_values, o_log_prob, gae, targets, state)
    updates, optimizer = solver.update(grads, optimizer, params)
    params = optax.apply_updates(params, updates)
    return params, optimizer, (loss, value_loss, loss_actor, entropy, state)

from functools import partial

def get_jit_ppo(config):
    return jax.jit(partial(ppo_update, config=config), static_argnums=(0, 1), donate_argnums=(2, 3))